---
layout: post
title: Bootstrap--Why It Works(III)
---

This post will take the viewpoint of **Bayesian inference** to look into why the bootstrap would work. Most of the content is borrowed from *The Element of Statistical Learning* (section 8.4 in first edition).

## Parametric bootstrap

This example is a little trivial, but under this setting parametric bootstrap can be thought of as a Bayesian posterior inference.

Suppose we observe a single observation $$z$$ from a normal distribution 
$$ z|\theta \sim N(\theta, 1)$$. A conjugate prior for $$\theta$$ is a normal distribution $$\theta\sim N(0, \tau)$$. The posterior can be obtained as 

$$\theta|z \sim N(z, \dfrac{1}{1+1/\tau})$$

Let  $$\tau \to \infty$$, then the prior become flat or non-informative, and $$\theta|z \sim N(z, 1)$$. 

The posterior inference for this Bayesian model has the same form of parametric bootstrap: we specify a parametric model first, then plug in the unknown parameters with maximum likelihood estimates (in this case the MLE is $$z$$), then repeatedly sample data from this distribution.

## Nonparametric bootstrap

Next, we will interpret the nonparametric bootstrap as a Bayesian model.

Suppose the a set of data $$x_1, x_2,... x_n$$ has $$L$$ distinct values, each having probability $$w_j$$. Actually, no matter whether the random variable is continuous or discrete, the observed data are still in a discrete scale. Let $$\hat w_j$$ be the observed proportion for the $$j$$-th value occurring in the data.  Let $$w = (w_1, w_2,..., w_L)$$ and $$\hat w = (\hat w_1, \hat w_2,... \hat w_L)$$. 

In the nonparametric bootstrap, the resampled data can be organized according to the $$L$$ distinct values. For the $$j$$-th value, the count of its occurrence is $$n\hat w ^*_j$$, where $$\hat w ^*_j$$ is the observed proportion in the resampled data. Then the count of occurrences for the $$L$$ values, arranged in a vector  $$n\hat w ^*$$ in the resample has a multinomial distribution $$ n\hat w ^* \sim \text{Multinomial}(n, \hat w)$$, and the mean is $$n\hat w $$, and the covariance for two components is $$ -n\hat w_i \hat w_j$$. It follows immediately that for $$\hat w^*$$, the mean is $$\hat w $$, and covariance is $$-\hat w_i \hat w_j/n$$.

Let's turn to some Bayesian view now. The likelihood of the data is proportional to $$w_1^{n\hat w_1} \ldots w_L^{n\hat w_L}$$. Giving a conjugate Dirichlet prior to $$w_1, \ldots, w_L$$ where all components have parameter $$a$$. Then the posterior is proportional to $$w_1^{n\hat w_1+a-1} \ldots w_L^{n\hat w_L+a-1}$$, which is recognized as the kernel of the Dirichlet$$(n\hat w_1+a, \ldots, n\hat w_L+a) $$ density. Let $$a = 0$$ to make the prior non-informative. Then the posterior for $$w$$ reduces to Dirichlet$$(n\hat w_1, \ldots, n\hat w_L) $$ who has mean $$\hat w$$ and covariance $$-\hat w_i \hat w_j/(n+1)$$. 

Putting everything together, the bootstrap distribution for $$\hat w ^*$$ is very similar to the posterior of $$w$$. That is, nonparametric bootstrapping is just taking samples from a posterior without even worrying about the Bayesian model specification at all.

I personally don't tend to think what was introduced above is about **connection**. Like a lot of other Bayesian models, they were proposed to **match** some existing methods through the Bayesian's route, at the end of which there is a delicate probabilistic model. 

---
### References

Trevor Hastie, Robert Tibshirani, and Jerome Friedman. *The elements of statistical learning*. New York, NY, USA:: Springer series in statistics, 2001. 