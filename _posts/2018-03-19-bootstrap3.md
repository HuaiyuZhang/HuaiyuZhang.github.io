---
layout: post
title: Bootstrap--Why It Works(III)
---

This post will take the viewpoint of **Bayesian inference** to discuss why the bootstrap would work. Most of the content is borrowed from *The Element of Statistical Learning*.

## Parametric bootstrap

This example is a little trivial, but it shows that parametric bootstrap can be thought of as a Bayesian posterior inference.

Suppose we observe a single observation $$z$$ from a normal distribution $$z|\theta \sim N(\theta, 1)$$. A conjugate prior for $\theta$ is a normal distribution $$\theta\sim N(0, \tau)$$. The posterior can be easily obtained as 

$$\theta|z \sim N(z, \dfrac{1}{1+1/\tau})$$

Let  $$\tau \to \infty$$, then the prior become flat or non-informative. Now $$\theta|z \sim N(z, 1)$$. 

The posterior inference for this Bayesian model has the same form of parametric bootstrap: we specify a parametric model first, then plug in the unknown parameters with maximum likelihood estimates (in this case the MLE is $$z$$), then repeatedly sample data from this distribution.

## Nonparametric bootstrap

Next, we will look at the correspondence of the nonparametric bootstrap with a Bayesian model.

Suppose the a discrete random variable $$z$$ has $$L$$ distinct values, each having probability $$w_j$$. Let $$\hat w_j$$ be the observed proportion for the $$j$$-th value occurring in the data.  Let $$w = (w_1, w_2,..., w_L)$$ and $$\hat w = (\hat w_1, \hat w_2,... \hat w_L)$$. 







The example in last section is well designed, in the sense that it requires strict conditions to make it work. 

---
### References

Trevor Hastie, Robert Tibshirani, and Jerome Friedman. *The elements of statistical learning*. New York, NY, USA:: Springer series in statistics, 2001.  Section 8.3