---
layout: post
title: Bootstrap--Why It Works
---

Bootstrap is one of the most ingenious innovation of statistics in the 20th century, especially prosperous with the aid of growing computation power. We heard and use it very often, but, why it works? This post will try to explore a reason behind the curtain. 

Bootstrap is often used for bias and standard error estimation. We will only look at a **standard error estimation** problem along with this text, but it can be generalized to other statistics of interest. Most of the materials are adapted from Tibshrani and Efron (2017). More serious treatment on this topic can be found in Efron (1982).

## Bootstrap and Jackknife

Suppose we have collected an iid sample $$ \mathbf{x} = (x_1, \ldots, x_n) $$ from a population $$F$$. Each $$x_i$$ is a univariate observation. The statistic being interested is denoted by $$\hat{\theta} = s(\mathbf{x})$$, i.e., feeding data into some known function $$s(.)$$ we can get the statistic. For example, a very common example,  $$\hat{\theta} = \bar{x}= \sum_{i = 1}^{n}x_i/n$$.

The bootstrap sample is generated by ***resampling***: draw $$n$$ observations, denoted by $$\mathbf{x^*} = (x_1^*, \ldots, x_n^*)$$, from the observed data, $$ x_1, \ldots, x_n $$, with replacement. In each round, it is able to compute $$ \hat{\theta}^*=s(\mathbf{x^*})$$. Repeat $$B$$ times, then the distribution formed by $$B$$ many $$\hat{\theta}^*$$ values is the distribution we want to use for the inference of $$\hat{\theta}$$. Want to the standard error? Just one step away, as what is usually done when computing the sample standard deviation:


$$
\hat{\text{se}}_{boot} = \left[ \frac{1}{B}\sum_{b=1}^{B} ( \hat{\theta}^{*}_{(b)}- \hat{\theta}^{*}_{(.)} )^2 \right]^{1/2}
$$

where $$\hat{\theta}^{*}_{(.)}= \frac{1}{B}\sum_{b=1}^{B} \hat{\theta}^{*}_{(b)} $$.

A closely related algorithm is called **Jackknife**. Denote $$\mathbf x_{(i)}= (x_1, \ldots, x_{i-1}, x_{i+1}, \ldots, x_n)$$ as a sample of size $$n-1$$ that is formed by discarding the $$i^{th}$$ observation, then the corresponding statistic is $$ \hat \theta_{(i)} = s(\mathbf x_{(i)})$$.

The Jackknife estimate for standard error of $$\hat \theta$$ is 

$$\hat{\text{se}}_\text{Jackknife} = \left[\frac{n-1}{n}\sum_{i = 1}^{n}(\hat \theta_{(i)} - \hat \theta_{(.)})^2\right]^{1/2}$$ 

where $$\hat{\theta}_{(.)}= \frac{1}{n}\sum_{i=1}^{n} \hat{\theta}{(i)} $$.



## Delta method

If $$\hat{\theta}$$, the statistic, is simply $$\overline{x}$$, the standard error formula can be easily found in any intro level stat textbook. But what if the form of $$\hat{\theta}$$ is very complicated, for example, the correlation coefficient,

$$
\hat{\theta} = s(\mathbf x)  = \sum_{i = 1}^{n}(x_i - \bar x)(y_i - \bar y)/\sqrt{\sum_{i = 1}^{n}(x_i - \bar x)^2\sum_{i = 1}^{n}(y_i - \bar y)^2}.
$$

Knowledgeable readers may have already come up with the answer, Delta method. Yes, if the statistic is a smooth function of the sample average, say $$\hat \theta = g(\bar x) $$, Delta method is a possible route and it goes as

$$
\hat{\text{se}}_{\text{Delta}}= \left[\nabla ^T  n^{-1}\Sigma  \nabla\right]^{1/2}
$$

where $$\nabla$$ is the first derivate of $$g$$, $$\Sigma$$ is the covariance matrix of $$x$$. Note that $$x$$ can be multivariate now. For practical use, all the quantities need to be evaluated with data.

For example, following [this note](http://personal.psu.edu/drh20/asymp/fall2002/lectures/ln08.pdf), one can get the standard error estimate for **correlation coefficient** :


$$
\hat{\text{se}}_{\text{Delta}}=\left\{ \frac{\hat \theta^2}{n^2}
\left[ \frac{\hat \mu_{40}}{\hat \mu_{20}^2} + \frac{\hat \mu_{04}}{\hat \mu_{02}^2} +
\frac{\hat 2\mu_{22}}{\hat \mu_{20}\hat \mu_{02}} +  \frac{4\hat \mu_{22}}{\hat \mu_{11}^2} -  \frac{4\hat \mu_{31}}{\hat \mu_{11} \hat \mu_{20} }- \frac{4\hat \mu_{13}}{\hat \mu_{11} \hat \mu_{02}}
\right]
\right\}^{1/2}
$$



where $$\hat \mu_{kl}= \sum_{i = 1}^{n} (x_i - \bar x)^k(y_i-\bar y)^l$$. 

The reason why I display the formula above is to show how messy it could be--it could be even worse in some other cases. Deriving such a result demands time and carefulness because multivariate derivatives arise.


## An interesting connection

It is well known that the Delta method relies on Taylor series, in whose core the first-order derivative plays the role. On the other side, the Jackknife estimate can be rewritten as 

$$\hat{\text{se}}_{\text{Jackknife}} = \left[ \sum_{i=1}^{n} \hat d_i^2/n^2\right]^{1/2}$$, where $$\hat d_i = \dfrac{\hat \theta_{(i)}- \hat \theta_{(.)}}{1/\sqrt{n(n-1)}}$$ . 

Roughly, $$\hat d_i$$ approximates the derivative of $$\hat \theta$$ w.r.t. the $$i^{th}$$ observation $$x_i$$. To make this point more clear requires more advanced notations. Let's just skip those complication, and grasp the essence here: instead of the derivative formula, Jackknife works out the derivative numerically. Efron (1981) showed that *infinitesimal Jackknife*, a slightly modified version of Jackknife, is exactly equivalent to Delta method. In short, Jackknife is a computational implementation of Delta method.

The story almost ends, but where is Bootstrap?  

Bootstrap shares much similarity with Jackknife in the algorithm. Rather than dealing with the derivative one direction a time, Bootstrap randomly picks some directions ($$x_i$$ values) and do the derivative. In fact, this feature brings benefit because it prevents the algorithm being addicted to local behavior of the data.  

## Final remark

The viewpoint from numerical derivative only provides one way of understanding Bootstrap, while another popular and more strict theoretical scheme is via the Edgeworth expansion (Hall, 1992). But that would be another topic. The point of this post is the fascinating connection with Delta method.

---
### References

Efron, Bradley. "Nonparametric estimates of standard error: the jackknife, the bootstrap and other methods." *Biometrika*68, no. 3 (1981): 589-599.

Efron, Bradley. *The jackknife, the bootstrap, and other resampling plans*. Vol. 38. Siam, (1982). Chapter 6.

Efron, B., & Hastie, T. *Computer Age Statistical Inference: Algorithms, Evidence, and Data Science* (Institute of Mathematical Statistics Monographs). Cambridge: Cambridge University Press. (2016). Chapter 10.

Hall, Peter. *The bootstrap and Edgeworth expansion*. Springer Science & Business Media, 2013.